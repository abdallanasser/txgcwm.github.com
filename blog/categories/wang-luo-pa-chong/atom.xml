<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 网络爬虫 | 残剑]]></title>
  <link href="http://txgcwm.github.io/blog/categories/wang-luo-pa-chong/atom.xml" rel="self"/>
  <link href="http://txgcwm.github.io/"/>
  <updated>2014-05-22T00:07:16+08:00</updated>
  <id>http://txgcwm.github.io/</id>
  <author>
    <name><![CDATA[残剑]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Scrapy入门简略教程（一）]]></title>
    <link href="http://txgcwm.github.io/blog/2014/04/28/scrapyru-men-jian-lue-jiao-cheng-%5B%3F%5D/"/>
    <updated>2014-04-28T23:04:00+08:00</updated>
    <id>http://txgcwm.github.io/blog/2014/04/28/scrapyru-men-jian-lue-jiao-cheng-[?]</id>
    <content type="html"><![CDATA[<p>在本教程中，将假设Scrapy已经安装在你的系统上。如果不是，请参见官方的安装指南。我将使用我自己的例子来介绍，主要完成以下任务：</p>

<ol>
<li>创建一个新的Scrapy项目；</li>
<li>定义你将提取的Items；</li>
<li>写一个蜘蛛抓取网站并提取Items；</li>
<li>写一个项目管道存储所提取的Items。</li>
</ol>


<p>Scrapy是用Python编写的，如果不了解的话，需要去学习一下，从项目上入手的话，应该学起来比较的快。</p>

<h1>创建一个项目</h1>

<p>在开始之前，需要在你的目录下创建一个项目，执行以下命令即可：
<code>
$ scrapy startproject shiyifang
</code></p>

<p>在当前的目录下就会生成一个叫<code>shiyifang</code>的文件夹，它包含以下一些文件和目录：
```
$ tree
.
├── scrapy.cfg
└── shiyifang</p>

<pre><code>├── __init__.py
├── items.py
├── pipelines.py
├── settings.py
└── spiders
    └── __init__.py
</code></pre>

<p>2 directories, 6 files
```</p>

<p>关于该目录下文件和文件夹的说明：</p>

<ul>
<li>scrapy.cfg: 项目的配置文件。</li>
<li>shiyifang/: 项目的python模块，后续会将代码放入这个目录。</li>
<li>shiyifang/items.py: 项目的items文件。</li>
<li>shiyifang/pipelines.py: 项目的pipelines文件。</li>
<li>shiyifang/settings.py: 项目的settings文件。</li>
<li>shiyifang/spiders/: 存放爬虫程序的目录。</li>
</ul>


<!--more-->


<h1>定义Item</h1>

<p>Items包含了将要被抓取的数据，它们就像简单的Python dicts，但对未填充字段提供额外的保护，以防止错误。在shiyifang目录下编辑items.py文件，Item类看起来像如下的样子：
```
from scrapy.item import Item, Field</p>

<p>class ShiyifangItem(Item):</p>

<pre><code># define the fields for your item here like:
# name = Field()
title = Field()
link = Field()
desc = Field()
</code></pre>

<p>```</p>

<h1>第一个爬虫</h1>

<p>爬虫是用户自己写的一个类，用于爬取一个或一组域名的数据，需要定义一个初始化的URL列表，要完成如何抓取链接和如何去解析数据等基本的工作。我们在shiyifang/spiders目录下创建一个叫shiyifang_spider.py文件：
```
from scrapy.spider import BaseSpider</p>

<p>class ShiyifangSpider(BaseSpider):</p>

<pre><code>name = "shiyifang"
allowed_domains = ["s.taobao.com"]
start_urls = [
    "http://s.taobao.com/search?q=%E6%A0%B8%E6%A1%83"
]

def parse(self, response):
    filename = response.url.split("/")[-2]
    open(filename, 'wb').write(response.body)
</code></pre>

<p>```</p>

<h1>Crawling</h1>

<p>到项目的顶层目录，执行以下的指令：
```
$ scrapy crawl shiyifang
2014-04-28 23:01:34+0800 [scrapy] INFO: Scrapy 0.20.0 started (bot: shiyifang)
2014-04-28 23:01:34+0800 [scrapy] DEBUG: Optional features available: ssl, http11
2014-04-28 23:01:34+0800 [scrapy] DEBUG: Overridden settings: {&lsquo;NEWSPIDER_MODULE&rsquo;: &lsquo;shiyifang.spiders&rsquo;, &lsquo;SPIDER_MODULES&rsquo;: [&lsquo;shiyifang.spiders&rsquo;], &lsquo;BOT_NAME&rsquo;: &lsquo;shiyifang&rsquo;}
2014-04-28 23:01:35+0800 [scrapy] DEBUG: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2014-04-28 23:01:35+0800 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2014-04-28 23:01:35+0800 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2014-04-28 23:01:35+0800 [scrapy] DEBUG: Enabled item pipelines:
2014-04-28 23:01:35+0800 [shiyifang] INFO: Spider opened
2014-04-28 23:01:35+0800 [shiyifang] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2014-04-28 23:01:35+0800 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023
2014-04-28 23:01:35+0800 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080
2014-04-28 23:01:36+0800 [shiyifang] DEBUG: Crawled (200) <GET http://s.taobao.com/search?q=%E6%A0%B8%E6%A1%83> (referer: None)
2014-04-28 23:01:36+0800 [shiyifang] INFO: Closing spider (finished)
2014-04-28 23:01:36+0800 [shiyifang] INFO: Dumping Scrapy stats:</p>

<pre><code>{'downloader/request_bytes': 245,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 34569,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2014, 4, 28, 15, 1, 36, 232914),
 'log_count/DEBUG': 7,
 'log_count/INFO': 3,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2014, 4, 28, 15, 1, 35, 615729)}
</code></pre>

<p>2014-04-28 23:01:36+0800 [shiyifang] INFO: Spider closed (finished)
```
从debug信息可以看出，我们发送的get请求是收到回复的，且是成功的。此时，在当前目录下你会看到一个叫s.taobao.com的文件生成，这就是我们抓取到的网页。</p>

<p><big>参考文章</big></p>

<p><a href="http://doc.scrapy.org/en/latest/intro/tutorial.html">Scrapy Tutorial</a><br/>
<a href="http://blog.pluskid.org/?p=366">Scrapy 轻松定制网络爬虫</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[运行Scrapy项目出错：KeyError: 'Spider not found: tutorial']]></title>
    <link href="http://txgcwm.github.io/blog/2014/04/27/yun-xing-scrapyxiang-mu-chu-cuo/"/>
    <updated>2014-04-27T22:40:00+08:00</updated>
    <id>http://txgcwm.github.io/blog/2014/04/27/yun-xing-scrapyxiang-mu-chu-cuo</id>
    <content type="html"><![CDATA[<p>运行Scrapy抓取网上数据的时候，出现了以下的错误：
```
$ sudo scrapy crawl tutorial
Traceback (most recent call last):
  File &ldquo;/usr/local/bin/scrapy&rdquo;, line 4, in <module></p>

<pre><code>execute()
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py&rdquo;, line 143, in execute</p>

<pre><code>_run_print_help(parser, _run_command, cmd, args, opts)
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py&rdquo;, line 89, in _run_print_help</p>

<pre><code>func(*a, **kw)
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py&rdquo;, line 150, in _run_command</p>

<pre><code>cmd.run(args, opts)
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py&rdquo;, line 48, in run</p>

<pre><code>spider = crawler.spiders.create(spname, **opts.spargs)
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/spidermanager.py&rdquo;, line 44, in create</p>

<pre><code>raise KeyError("Spider not found: %s" % spider_name)
</code></pre>

<p>KeyError: &lsquo;Spider not found: tutorial&rsquo;
```</p>

<!--more-->


<p>查看<code>tutorial/spiders/dmoz_spider.py</code>文件中的内容如下：
```
$ cat tutorial/spiders/dmoz_spider.py
from scrapy.spider import BaseSpider</p>

<p>class DmozSpider(BaseSpider):</p>

<pre><code>name = "dmoz"
allowed_domains = ["s.taobao.com"]
start_urls = [
    "http://s.taobao.com/search?q=%E6%A0%B8%E6%A1%83"
]

def parse(self, response):
    filename = response.url.split("/")[-2]
    open(filename, 'wb').write(response.body)
</code></pre>

<p>```</p>

<p><code>name = "dmoz"</code>中的<code>dmoz</code>才是crawler的名字。修改crawler的名字后，正常运行如下：
```
$ sudo scrapy crawl dmoz
2014-04-27 22:50:09+0800 [scrapy] INFO: Scrapy 0.20.0 started (bot: tutorial)
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Optional features available: ssl, http11
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Overridden settings: {&lsquo;NEWSPIDER_MODULE&rsquo;: &lsquo;tutorial.spiders&rsquo;, &lsquo;SPIDER_MODULES&rsquo;: [&lsquo;tutorial.spiders&rsquo;], &lsquo;BOT_NAME&rsquo;: &lsquo;tutorial&rsquo;}
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Enabled item pipelines:
2014-04-27 22:50:09+0800 [dmoz] INFO: Spider opened
2014-04-27 22:50:09+0800 [dmoz] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080
2014-04-27 22:50:10+0800 [dmoz] DEBUG: Crawled (200) <GET http://s.taobao.com/search?q=%E6%A0%B8%E6%A1%83> (referer: None)
2014-04-27 22:50:10+0800 [dmoz] INFO: Closing spider (finished)
2014-04-27 22:50:10+0800 [dmoz] INFO: Dumping Scrapy stats:</p>

<pre><code>{'downloader/request_bytes': 245,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 35012,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2014, 4, 27, 14, 50, 10, 637578),
 'log_count/DEBUG': 7,
 'log_count/INFO': 3,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2014, 4, 27, 14, 50, 9, 940145)}
</code></pre>

<p>2014-04-27 22:50:10+0800 [dmoz] INFO: Spider closed (finished)
```</p>

<p><big>参考文章：</big></p>

<p><a href="http://www.crifan.com/python_run_scrapy_keyerror_spider_not_found/">运行Scrapy项目结果出错：KeyError: ‘Spider not found: manta’</a><br/>
<a href="http://www.crifan.com/try_scrapy_tutorial/">折腾Scrapy的Tutorial</a><br/>
<a href="http://stackoverflow.com/questions/9876793/scrapy-spider-not-found-error">Scrapy spider not found error</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu12.10安装Scrapy]]></title>
    <link href="http://txgcwm.github.io/blog/2013/11/13/ubuntu12-dot-10an-zhuang-scrapy/"/>
    <updated>2013-11-13T15:59:00+08:00</updated>
    <id>http://txgcwm.github.io/blog/2013/11/13/ubuntu12-dot-10an-zhuang-scrapy</id>
    <content type="html"><![CDATA[<p>根据官网<a href="http://doc.scrapy.org/en/latest/topics/ubuntu.html#topics-ubuntu">Ubuntu packages</a>上的方法安装Scrapy，然后创建一个教程目录，却出现了一些问题，如下所示。</p>

<p>```
$ scrapy startproject tutorial<br/>
Traceback (most recent call last):<br/>
  File &ldquo;/usr/bin/scrapy&rdquo;, line 4, in <module></p>

<pre><code>execute()  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/cmdline.py&rdquo;, line 121, in execute</p>

<pre><code>cmds = _get_commands_dict(settings, inproject)  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/cmdline.py&rdquo;, line 45, in _get_commands_dict</p>

<pre><code>cmds = _get_commands_from_module('scrapy.commands', inproject)  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/cmdline.py&rdquo;, line 28, in _get_commands_from_module</p>

<pre><code>for cmd in _iter_command_classes(module):  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/cmdline.py&rdquo;, line 19, in _iter_command_classes</p>

<pre><code>for module in walk_modules(module_name):  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/utils/misc.py&rdquo;, line 66, in walk_modules</p>

<pre><code>submod = __import__(fullpath, {}, {}, [''])  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/commands/deploy.py&rdquo;, line 13, in <module></p>

<pre><code>from w3lib.form import encode_multipart  
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/w3lib/form.py&rdquo;, line 2, in <module></p>

<pre><code>if six.PY2:  
</code></pre>

<p>AttributeError: &lsquo;module&rsquo; object has no attribute &lsquo;PY2'<br/>
```</p>

<!--more-->


<p>从网上找到了一个答案，需要利用pip去安装。</p>

<p>```
$ pip install Scrapy<br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): Scrapy in   /usr/lib/pymodules/python2.7 <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): Twisted>=8.0 in   /usr/lib/python2.7/dist-packages (from Scrapy)<br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): w3lib>=1.2 in   /usr/lib/python2.7/dist-packages (from Scrapy)<br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): queuelib in   /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): lxml in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): pyOpenSSL in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Downloading/unpacking six>=1.4.1 (from w3lib>=1.2->Scrapy) <br/>
  Downloading six-1.4.1.tar.gz <br/>
  Running setup.py egg_info for package six</p>

<p>Installing collected packages: six <br/>
  Found existing installation: six 1.1.0</p>

<pre><code>Uninstalling six:   
</code></pre>

<p>Exception: <br/>
Traceback (most recent call last): <br/>
  File &ldquo;/usr/lib/python2.7/dist-packages/pip/basecommand.py&rdquo;, line 104, in main</p>

<pre><code>status = self.run(options, args)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/commands/install.py&rdquo;, line 250, in run</p>

<pre><code>requirement_set.install(install_options, global_options)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/req.py&rdquo;, line 1129, in install</p>

<pre><code>requirement.uninstall(auto_confirm=True)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/req.py&rdquo;, line 486, in uninstall</p>

<pre><code>paths_to_remove.remove(auto_confirm)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/req.py&rdquo;, line 1431, in remove</p>

<pre><code>renames(path, new_path)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/util.py&rdquo;, line 263, in renames</p>

<pre><code>shutil.move(old, new)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/shutil.py&rdquo;, line 302, in move</p>

<pre><code>os.unlink(src)   
</code></pre>

<p>OSError: [Errno 13] Permission denied: &lsquo;/usr/share/pyshared/six-1.1.0.egg-info&rsquo;</p>

<p>Storing complete log in /home/txgcwm/.pip/pip.log <br/>
```</p>

<p>借助新得立把six给卸载掉，然后重新安装。</p>

<p>```
$ sudo pip install Scrapy<br/>
Downloading/unpacking Scrapy <br/>
  Downloading Scrapy-0.20.0.tar.gz (745Kb): 745Kb downloaded <br/>
  Running setup.py egg_info for package Scrapy</p>

<pre><code>no previously-included directories found matching 'docs/build'   
</code></pre>

<p>Requirement already satisfied (use &mdash;upgrade to upgrade): Twisted>=10.0.0 in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Downloading/unpacking w3lib>=1.2 (from Scrapy) <br/>
  Downloading w3lib-1.5.tar.gz <br/>
  Running setup.py egg_info for package w3lib</p>

<p>Requirement already satisfied (use &mdash;upgrade to upgrade): queuelib in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): lxml in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): pyOpenSSL in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Downloading/unpacking cssselect>=0.9 (from Scrapy) <br/>
  Downloading cssselect-0.9.1.tar.gz <br/>
  Running setup.py egg_info for package cssselect</p>

<pre><code>no previously-included directories found matching 'docs/_build'   
</code></pre>

<p>Downloading/unpacking six>=1.4.1 (from w3lib>=1.2->Scrapy) <br/>
  Running setup.py egg_info for package six</p>

<p>Installing collected packages: Scrapy, w3lib, cssselect, six <br/>
  Running setup.py install for Scrapy</p>

<pre><code>changing mode of build/scripts-2.7/scrapy from 644 to 755   

no previously-included directories found matching 'docs/build'   
changing mode of /usr/local/bin/scrapy to 755   
</code></pre>

<p>  Running setup.py install for w3lib</p>

<p>  Running setup.py install for cssselect</p>

<pre><code>no previously-included directories found matching 'docs/_build'   
</code></pre>

<p>  Running setup.py install for six</p>

<p>Successfully installed Scrapy w3lib cssselect six <br/>
Cleaning up&hellip; <br/>
```</p>

<p>创立一个项目。</p>

<p><code>
$ sudo scrapy startproject tutorial   
</code></p>
]]></content>
  </entry>
  
</feed>
