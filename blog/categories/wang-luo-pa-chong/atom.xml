<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 网络爬虫 | 残剑]]></title>
  <link href="http://txgcwm.github.io/blog/categories/wang-luo-pa-chong/atom.xml" rel="self"/>
  <link href="http://txgcwm.github.io/"/>
  <updated>2014-04-27T23:49:19+08:00</updated>
  <id>http://txgcwm.github.io/</id>
  <author>
    <name><![CDATA[残剑]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[运行Scrapy项目出错：KeyError: 'Spider not found: tutorial']]></title>
    <link href="http://txgcwm.github.io/blog/2014/04/27/yun-xing-scrapyxiang-mu-chu-cuo/"/>
    <updated>2014-04-27T22:40:00+08:00</updated>
    <id>http://txgcwm.github.io/blog/2014/04/27/yun-xing-scrapyxiang-mu-chu-cuo</id>
    <content type="html"><![CDATA[<p>运行Scrapy抓取网上数据的时候，出现了以下的错误：
```
$ sudo scrapy crawl tutorial
Traceback (most recent call last):
  File &ldquo;/usr/local/bin/scrapy&rdquo;, line 4, in <module></p>

<pre><code>execute()
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py&rdquo;, line 143, in execute</p>

<pre><code>_run_print_help(parser, _run_command, cmd, args, opts)
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py&rdquo;, line 89, in _run_print_help</p>

<pre><code>func(*a, **kw)
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py&rdquo;, line 150, in _run_command</p>

<pre><code>cmd.run(args, opts)
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py&rdquo;, line 48, in run</p>

<pre><code>spider = crawler.spiders.create(spname, **opts.spargs)
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/spidermanager.py&rdquo;, line 44, in create</p>

<pre><code>raise KeyError("Spider not found: %s" % spider_name)
</code></pre>

<p>KeyError: &lsquo;Spider not found: tutorial&rsquo;
```</p>

<!--more-->


<p>查看<code>tutorial/spiders/dmoz_spider.py</code>文件中的内容如下：
```
$ cat tutorial/spiders/dmoz_spider.py
from scrapy.spider import BaseSpider</p>

<p>class DmozSpider(BaseSpider):</p>

<pre><code>name = "dmoz"
allowed_domains = ["s.taobao.com"]
start_urls = [
    "http://s.taobao.com/search?q=%E6%A0%B8%E6%A1%83"
]

def parse(self, response):
    filename = response.url.split("/")[-2]
    open(filename, 'wb').write(response.body)
</code></pre>

<p>```</p>

<p><code>name = "dmoz"</code>中的<code>dmoz</code>才是crawler的名字。修改crawler的名字后，正常运行如下：
```
$ sudo scrapy crawl dmoz
2014-04-27 22:50:09+0800 [scrapy] INFO: Scrapy 0.20.0 started (bot: tutorial)
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Optional features available: ssl, http11
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Overridden settings: {&lsquo;NEWSPIDER_MODULE&rsquo;: &lsquo;tutorial.spiders&rsquo;, &lsquo;SPIDER_MODULES&rsquo;: [&lsquo;tutorial.spiders&rsquo;], &lsquo;BOT_NAME&rsquo;: &lsquo;tutorial&rsquo;}
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Enabled item pipelines:
2014-04-27 22:50:09+0800 [dmoz] INFO: Spider opened
2014-04-27 22:50:09+0800 [dmoz] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080
2014-04-27 22:50:10+0800 [dmoz] DEBUG: Crawled (200) <GET http://s.taobao.com/search?q=%E6%A0%B8%E6%A1%83> (referer: None)
2014-04-27 22:50:10+0800 [dmoz] INFO: Closing spider (finished)
2014-04-27 22:50:10+0800 [dmoz] INFO: Dumping Scrapy stats:</p>

<pre><code>{'downloader/request_bytes': 245,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 35012,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2014, 4, 27, 14, 50, 10, 637578),
 'log_count/DEBUG': 7,
 'log_count/INFO': 3,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2014, 4, 27, 14, 50, 9, 940145)}
</code></pre>

<p>2014-04-27 22:50:10+0800 [dmoz] INFO: Spider closed (finished)
```</p>

<p><big>参考文章：</big></p>

<p><a href="http://www.crifan.com/python_run_scrapy_keyerror_spider_not_found/">运行Scrapy项目结果出错：KeyError: ‘Spider not found: manta’</a><br/>
<a href="http://www.crifan.com/try_scrapy_tutorial/">折腾Scrapy的Tutorial</a><br/>
<a href="http://stackoverflow.com/questions/9876793/scrapy-spider-not-found-error">Scrapy spider not found error</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu12.10安装Scrapy]]></title>
    <link href="http://txgcwm.github.io/blog/2013/11/13/ubuntu12-dot-10an-zhuang-scrapy/"/>
    <updated>2013-11-13T15:59:00+08:00</updated>
    <id>http://txgcwm.github.io/blog/2013/11/13/ubuntu12-dot-10an-zhuang-scrapy</id>
    <content type="html"><![CDATA[<p>根据官网<a href="http://doc.scrapy.org/en/latest/topics/ubuntu.html#topics-ubuntu">Ubuntu packages</a>上的方法安装Scrapy，然后创建一个教程目录，却出现了一些问题，如下所示。</p>

<p>```
$ scrapy startproject tutorial<br/>
Traceback (most recent call last):<br/>
  File &ldquo;/usr/bin/scrapy&rdquo;, line 4, in <module></p>

<pre><code>execute()  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/cmdline.py&rdquo;, line 121, in execute</p>

<pre><code>cmds = _get_commands_dict(settings, inproject)  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/cmdline.py&rdquo;, line 45, in _get_commands_dict</p>

<pre><code>cmds = _get_commands_from_module('scrapy.commands', inproject)  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/cmdline.py&rdquo;, line 28, in _get_commands_from_module</p>

<pre><code>for cmd in _iter_command_classes(module):  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/cmdline.py&rdquo;, line 19, in _iter_command_classes</p>

<pre><code>for module in walk_modules(module_name):  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/utils/misc.py&rdquo;, line 66, in walk_modules</p>

<pre><code>submod = __import__(fullpath, {}, {}, [''])  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/commands/deploy.py&rdquo;, line 13, in <module></p>

<pre><code>from w3lib.form import encode_multipart  
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/w3lib/form.py&rdquo;, line 2, in <module></p>

<pre><code>if six.PY2:  
</code></pre>

<p>AttributeError: &lsquo;module&rsquo; object has no attribute &lsquo;PY2'<br/>
```</p>

<!--more-->


<p>从网上找到了一个答案，需要利用pip去安装。</p>

<p>```
$ pip install Scrapy<br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): Scrapy in   /usr/lib/pymodules/python2.7 <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): Twisted>=8.0 in   /usr/lib/python2.7/dist-packages (from Scrapy)<br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): w3lib>=1.2 in   /usr/lib/python2.7/dist-packages (from Scrapy)<br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): queuelib in   /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): lxml in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): pyOpenSSL in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Downloading/unpacking six>=1.4.1 (from w3lib>=1.2->Scrapy) <br/>
  Downloading six-1.4.1.tar.gz <br/>
  Running setup.py egg_info for package six</p>

<p>Installing collected packages: six <br/>
  Found existing installation: six 1.1.0</p>

<pre><code>Uninstalling six:   
</code></pre>

<p>Exception: <br/>
Traceback (most recent call last): <br/>
  File &ldquo;/usr/lib/python2.7/dist-packages/pip/basecommand.py&rdquo;, line 104, in main</p>

<pre><code>status = self.run(options, args)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/commands/install.py&rdquo;, line 250, in run</p>

<pre><code>requirement_set.install(install_options, global_options)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/req.py&rdquo;, line 1129, in install</p>

<pre><code>requirement.uninstall(auto_confirm=True)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/req.py&rdquo;, line 486, in uninstall</p>

<pre><code>paths_to_remove.remove(auto_confirm)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/req.py&rdquo;, line 1431, in remove</p>

<pre><code>renames(path, new_path)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/util.py&rdquo;, line 263, in renames</p>

<pre><code>shutil.move(old, new)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/shutil.py&rdquo;, line 302, in move</p>

<pre><code>os.unlink(src)   
</code></pre>

<p>OSError: [Errno 13] Permission denied: &lsquo;/usr/share/pyshared/six-1.1.0.egg-info&rsquo;</p>

<p>Storing complete log in /home/txgcwm/.pip/pip.log <br/>
```</p>

<p>借助新得立把six给卸载掉，然后重新安装。</p>

<p>```
$ sudo pip install Scrapy<br/>
Downloading/unpacking Scrapy <br/>
  Downloading Scrapy-0.20.0.tar.gz (745Kb): 745Kb downloaded <br/>
  Running setup.py egg_info for package Scrapy</p>

<pre><code>no previously-included directories found matching 'docs/build'   
</code></pre>

<p>Requirement already satisfied (use &mdash;upgrade to upgrade): Twisted>=10.0.0 in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Downloading/unpacking w3lib>=1.2 (from Scrapy) <br/>
  Downloading w3lib-1.5.tar.gz <br/>
  Running setup.py egg_info for package w3lib</p>

<p>Requirement already satisfied (use &mdash;upgrade to upgrade): queuelib in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): lxml in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): pyOpenSSL in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Downloading/unpacking cssselect>=0.9 (from Scrapy) <br/>
  Downloading cssselect-0.9.1.tar.gz <br/>
  Running setup.py egg_info for package cssselect</p>

<pre><code>no previously-included directories found matching 'docs/_build'   
</code></pre>

<p>Downloading/unpacking six>=1.4.1 (from w3lib>=1.2->Scrapy) <br/>
  Running setup.py egg_info for package six</p>

<p>Installing collected packages: Scrapy, w3lib, cssselect, six <br/>
  Running setup.py install for Scrapy</p>

<pre><code>changing mode of build/scripts-2.7/scrapy from 644 to 755   

no previously-included directories found matching 'docs/build'   
changing mode of /usr/local/bin/scrapy to 755   
</code></pre>

<p>  Running setup.py install for w3lib</p>

<p>  Running setup.py install for cssselect</p>

<pre><code>no previously-included directories found matching 'docs/_build'   
</code></pre>

<p>  Running setup.py install for six</p>

<p>Successfully installed Scrapy w3lib cssselect six <br/>
Cleaning up&hellip; <br/>
```</p>

<p>创立一个项目。</p>

<p><code>
$ sudo scrapy startproject tutorial   
</code></p>
]]></content>
  </entry>
  
</feed>
