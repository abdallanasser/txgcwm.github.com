<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 网络爬虫 | 残剑]]></title>
  <link href="http://txgcwm.github.io/blog/categories/wang-luo-pa-chong/atom.xml" rel="self"/>
  <link href="http://txgcwm.github.io/"/>
  <updated>2015-12-15T23:41:46+08:00</updated>
  <id>http://txgcwm.github.io/</id>
  <author>
    <name><![CDATA[残剑]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Scrapy入门简略教程（二）]]></title>
    <link href="http://txgcwm.github.io/blog/2014/06/15/scrapyru-men-jian-lue-jiao-cheng-er/"/>
    <updated>2014-06-15T13:44:00+08:00</updated>
    <id>http://txgcwm.github.io/blog/2014/06/15/scrapyru-men-jian-lue-jiao-cheng-er</id>
    <content type="html"><![CDATA[<h1>What just happened under the hood?</h1>

<p>Scrapy为爬虫中的<code>start_urls</code>属性中每个URL创建了一个<code>scrapy.http.Request</code>对象 ，并把<code>parse</code>方法作为回调函数指定给爬虫。</p>

<p>这些<code>Request</code>会被调度，执行，<code>scrapy.http.Response</code>对象被返回，结果也被反馈给爬虫，之后通过<code>parse()</code>方法解析。</p>

<h1>Extracting Items</h1>

<h2>Introduction to Selectors</h2>

<p>有很多方法从网站中提取数据。Scrapy使用一种叫做Scrapy Selectors的机制，它基于XPath或者是CSS表达式。如果想了解更多selectors和其它机制，可查阅<a href="http://doc.scrapy.org/en/latest/topics/selectors.html#topics-selectors">Selectors documentation</a>。</p>

<p>以下是XPath表达式的例子和它们的含义：</p>

<ul>
<li><code>/html/head/title</code>: 选择HTML文档<code>&lt;head&gt;</code>元素下面的<code>&lt;title&gt;</code>标签。</li>
<li><code>/html/head/title/text()</code>: 选择前面提到的<code>&lt;title&gt;</code>元素下面的文本内容。</li>
<li><code>//td</code>: 选择所有 <code>&lt;td&gt;</code>元素。</li>
<li><code>//div[@class="mine"]</code>: 选择所有包含<code>class="mine"</code>属性的<code>div</code>标签元素。</li>
</ul>


<p>这只是几个使用XPath的简单例子，但是实际上XPath非常强大。如果你想了解更多XPATH的内容，我们向你推荐这个<a href="http://www.w3schools.com/XPath/default.asp">XPath教程</a>。</p>

<p>为了方便使用XPaths，Scrapy提供XPath<code>Selector</code>类， 有两种方式可以选择，<code>HtmlResponse</code>(HTML数据解析) 和<code>XmlResponse</code>(XML数据解析)。</p>

<p>使用XPath选择器，Scrapy提供了一个<code>Selector</code>类，它实例化一个<code>Htmlresponse</code>或<code>Xmlresponse</code>对象作为第一个参数。</p>

<p>你可以看到selectors作为对象表示在文档结构节点。所以，第一个实例化selectors是与根节点相关联，或整个文档。</p>

<p>Selectors有四种基本方法（具体的使用请查看API文档）：</p>

<ul>
<li><code>path()</code>：返回selectors列表, 每一个select表示一个xpath参数表达式选择的节点。</li>
<li><code>css()</code>: 返回selectors列表, 每一个select表示一个CSS参数表达式选择的节点。</li>
<li><code>extract()</code>：返回一个unicode字符串，该字符串为XPath选择器返回的数据。</li>
<li><code>re()</code>： 返回unicode字符串列表，字符串作为参数由正则表达式提取出来。</li>
</ul>


<!--more-->


<h2>Trying Selectors in the Shell</h2>

<p>为了演示Selectors的用法，我们将用到内建的Scrapy shell，这需要系统已经安装IPython (一个扩展的python交互环境)。</p>

<p>要开始shell，必须进入项目顶层目录，然后输入：
<code>
scrapy shell "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"
</code>
shell会输入以下的类似信息：
```
[ &hellip; Scrapy log here &hellip; ]</p>

<p>2014-01-23 17:11:42-0400 [default] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)
[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x3636b50>
[s]   item       {}
[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>
[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>
[s]   sel        <Selector xpath=None data=u'<html>\r\n<head>\r\n<meta http-equiv="Conten'>
[s]   settings   <CrawlerSettings module=None>
[s]   spider     <Spider 'default' at 0x3cebf50>
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser</p>

<p>In [1]:
```</p>

<p>Shell加载完毕后，将获得回应，这些内容被存储在本地变量<code>response</code>中，所以如果你输入<code>response.body</code>将会看到response的body部分，或者输入<code>response.headers</code>来查看它的header部分。</p>

<p>Shell通过变量<code>sel</code>为这个response提前初始化一个selector，这个selector基于response的类型自动选择最佳的解析规则（XML或者HTML）。</p>

<p>我们来看看里面有什么：</p>

<p>```
In [1]: sel.xpath(&lsquo;//title&rsquo;)
Out[1]: [<Selector xpath='//title' data=u'<title>Open Directory &ndash; Computers: Progr'>]</p>

<p>In [2]: sel.xpath(&lsquo;//title&rsquo;).extract()
Out[2]: [u'<title>Open Directory &ndash; Computers: Programming: Languages: Python: Books</title>&lsquo;]</p>

<p>In [3]: sel.xpath(&lsquo;//title/text()&rsquo;)
Out[3]: [<Selector xpath='//title/text()' data=u'Open Directory - Computers: Programming:'>]</p>

<p>In [4]: sel.xpath(&lsquo;//title/text()&rsquo;).extract()
Out[4]: [u'Open Directory &ndash; Computers: Programming: Languages: Python: Books']</p>

<p>In [5]: sel.xpath(&lsquo;//title/text()&rsquo;).re(&lsquo;(\w+):&rsquo;)
Out[5]: [u'Computers', u'Programming', u'Languages', u'Python']
```</p>

<h2>Extracting the data</h2>

<p>现在我们尝试从网页中提取一些真正的数据。</p>

<p>你可以在控制台输入<code>response.body</code>，检查源代码中的<code>XPaths</code>是否与预期相同。然而，检查HTML源代码是件很枯燥的事情。为了使事情变得简单，我们使用类似Firebug的Firefox扩展插件。更多信息请查看<a href="http://doc.scrapy.org/en/latest/topics/firebug.html#topics-firebug">Using Firebug for scraping</a>和<a href="http://doc.scrapy.org/en/latest/topics/firefox.html#topics-firefox">Using Firefox for scraping</a>。</p>

<p>检查源代码后，你会发现我们需要的数据在<code>&lt;ul&gt;</code>元素中，事实上是第二个<code>&lt;ul&gt;</code>。</p>

<p>我们可以通过如下命令选出每个站点中的<code>&lt;li&gt;</code>元素:</p>

<p><code>
sel.xpath('//ul/li')
</code>
站点的描述信息：
<code>
sel.xpath('//ul/li/text()').extract()
</code>
站点的标题：
<code>
sel.xpath('//ul/li/a/text()').extract()
</code>
站点的链接：
<code>
sel.xpath('//ul/li/a/@href').extract()
</code>
正如我们前面说的那样，每一个<code>.xpath</code>会返回一个selectors的列表，所以我们可以结合<code>.xpath()</code>去挖掘更深的节点。我们将会用到这些特性，所以:
```
sites = sel.xpath(&lsquo;//ul/li&rsquo;)
for site in sites:</p>

<pre><code>title = site.xpath('a/text()').extract()
link = site.xpath('a/@href').extract()
desc = site.xpath('text()').extract()
print title, link, desc
</code></pre>

<p><code>
我们把这些代码添加到爬虫中：
</code>
from scrapy.spider import Spider
from scrapy.selector import Selector</p>

<p>class DmozSpider(Spider):</p>

<pre><code>name = "dmoz"
allowed_domains = ["dmoz.org"]
start_urls = [
    "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
    "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
]

def parse(self, response):
    sel = Selector(response)
    sites = sel.xpath('//ul/li')
    for site in sites:
        title = site.xpath('a/text()').extract()
        link = site.xpath('a/@href').extract()
        desc = site.xpath('text()').extract()
        print title, link, desc
</code></pre>

<p><code>
我们通过scrapy.selector将Selector类导入及初始化一个Selector实例。现在可以指定我们的XPaths，如在shell中做的那样。我们尝试重新抓取dmoz.org站点，你将会在你的输出平台看到站点的一些信息，输入如下的命令：
</code>
scrapy crawl dmoz
```</p>

<h2>Using our item</h2>

<p>Item对象是用户自定义的python字典，你可以获取它们每个字段的值(即之前定义的类的属性)，它使用了标准的字典语法，如下所示:
```</p>

<blockquote><blockquote><blockquote><p>item = DmozItem()
item[&lsquo;title&rsquo;] = &lsquo;Example title&rsquo;
item[&lsquo;title&rsquo;]
&lsquo;Example title&rsquo;
<code>
Spiders希望将其抓取的数据存放到`Item`对象中。为了返回我们抓取的数据，Spider的最终代码应当是这样:
</code>
from scrapy.spider import Spider
from scrapy.selector import Selector</p></blockquote></blockquote></blockquote>

<p>from tutorial.items import DmozItem</p>

<p>class DmozSpider(Spider):</p>

<pre><code>name = "dmoz"
allowed_domains = ["dmoz.org"]
start_urls = [
    "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
    "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
]

def parse(self, response):
    sel = Selector(response)
    sites = sel.xpath('//ul/li')
    items = []
    for site in sites:
        item = DmozItem()
        item['title'] = site.xpath('a/text()').extract()
        item['link'] = site.xpath('a/@href').extract()
        item['desc'] = site.xpath('text()').extract()
        items.append(item)
    return items
</code></pre>

<p><code>
再次抓取dmoz.org站点的信息：
</code>
[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/></p>

<pre><code> {'desc': [u' - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\n],
  'link': [u'http://gnosis.cx/TPiP/'],
  'title': [u'Text Processing in Python']}
</code></pre>

<p>[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/></p>

<pre><code> {'desc': [u' - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\n'],
  'link': [u'http://www.informit.com/store/product.aspx?isbn=0130211192'],
  'title': [u'XML Processing with Python']}
</code></pre>

<p>```</p>

<h1>Storing the scraped data</h1>

<p>保存抓取的信息的最简单方法就是使用<a href="http://doc.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports">Feed exports</a>，使用如下的命令：
<code>
scrapy crawl dmoz -o items.json -t json
</code>
所有抓取的items将以JSON格式被保存在新生成的<code>items.json</code>文件中。</p>

<p>在小型的项目中（如这个教程中的例子），这些已经足够。然而，如果你想用抓取的items做更复杂的事情，你可以写一个<a href="http://doc.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline">Item Pipeline</a>(条目管道)。在项目创建的时候，一个专门用于条目管道的占位符文件随着items一起被建立，在<code>tutorial/pipelines.py</code>中。如果你只需要存取这些抓取后的items的话，就不需要去实现任何的条目管道。</p>

<p><big>注：</big>整个Scrapy的入门教程写得比较杂乱，包含了翻译部分和自己写的测试小例。</p>

<p><td></td>
<big>参考文章</big></p>

<p><a href="http://doc.scrapy.org/en/latest/intro/overview.html">Scrapy at a glance</a>     <br/>
<a href="http://www.cnblogs.com/txw1958/archive/2012/07/16/scrapy-tutorial.html">Scrapy入门教程</a><br/>
<a href="http://www.biaodianfu.com/mplementation-of-targeted-crawling-system.html">聚焦爬虫：定向抓取系统的实现方法</a><br/>
<a href="http://lujiesky.blog.51cto.com/332319/1328176">开源python网络爬虫框架Scrapy</a><br/>
<a href="http://www.yakergong.net/blog/archives/500">使用scrapy进行大规模抓取(一)</a><br/>
<a href="http://blog.csdn.net/pleasecallmewhy/article/details/19642329">Python网络爬虫（12）：爬虫框架Scrapy的第一个爬虫示例入门教程</a><br/>
<a href="http://www.searchtb.com/2011/07/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%8A%93%E5%8F%96%E9%9B%86%E7%BE%A4.html?spm=0.0.0.0.oD8o9v">快速构建实时抓取集群</a> <br/>
<a href="http://www.pythontab.com/html/2013/pythonhexinbiancheng_0814/541.html">python爬虫框架scrapy实例详解</a> <br/>
<a href="http://blog.pluskid.org/?p=366">Scrapy 轻松定制网络爬虫</a> <br/>
<a href="https://github.com/scrapinghub">Scrapinghub</a><br/>
<a href="http://www.kankanews.com/ICkengine/archives/94817.shtml">使用python，scrapy写（定制）爬虫的经验，资料，杂</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scrapy入门简略教程（一）]]></title>
    <link href="http://txgcwm.github.io/blog/2014/04/28/scrapyru-men-jian-lue-jiao-cheng-%5B%3F%5D/"/>
    <updated>2014-04-28T23:04:00+08:00</updated>
    <id>http://txgcwm.github.io/blog/2014/04/28/scrapyru-men-jian-lue-jiao-cheng-[?]</id>
    <content type="html"><![CDATA[<p>在本教程中，将假设Scrapy已经安装在你的系统上。如果不是，请参见官方的安装指南。我将使用我自己的例子来介绍，主要完成以下任务：</p>

<ol>
<li>创建一个新的Scrapy项目；</li>
<li>定义你将提取的Items；</li>
<li>写一个蜘蛛抓取网站并提取Items；</li>
<li>写一个项目管道存储所提取的Items。</li>
</ol>


<p>Scrapy是用Python编写的，如果不了解的话，需要去学习一下，从项目上入手的话，应该学起来比较的快。</p>

<h1>创建一个项目</h1>

<p>在开始之前，需要在你的目录下创建一个项目，执行以下命令即可：
<code>
$ scrapy startproject shiyifang
</code></p>

<p>在当前的目录下就会生成一个叫<code>shiyifang</code>的文件夹，它包含以下一些文件和目录：
```
$ tree
.
├── scrapy.cfg
└── shiyifang</p>

<pre><code>├── __init__.py
├── items.py
├── pipelines.py
├── settings.py
└── spiders
    └── __init__.py
</code></pre>

<p>2 directories, 6 files
```</p>

<p>关于该目录下文件和文件夹的说明：</p>

<ul>
<li>scrapy.cfg: 项目的配置文件。</li>
<li>shiyifang/: 项目的python模块，后续会将代码放入这个目录。</li>
<li>shiyifang/items.py: 项目的items文件。</li>
<li>shiyifang/pipelines.py: 项目的pipelines文件。</li>
<li>shiyifang/settings.py: 项目的settings文件。</li>
<li>shiyifang/spiders/: 存放爬虫程序的目录。</li>
</ul>


<!--more-->


<h1>定义Item</h1>

<p>Items包含了将要被抓取的数据，它们就像简单的Python dicts，但对未填充字段提供额外的保护，以防止错误。在shiyifang目录下编辑items.py文件，Item类看起来像如下的样子：
```
from scrapy.item import Item, Field</p>

<p>class ShiyifangItem(Item):</p>

<pre><code># define the fields for your item here like:
# name = Field()
title = Field()
link = Field()
desc = Field()
</code></pre>

<p>```</p>

<h1>第一个爬虫</h1>

<p>爬虫是用户自己写的一个类，用于爬取一个或一组域名的数据，需要定义一个初始化的URL列表，要完成如何抓取链接和如何去解析数据等基本的工作。我们在shiyifang/spiders目录下创建一个叫shiyifang_spider.py文件：
```
from scrapy.spider import BaseSpider</p>

<p>class ShiyifangSpider(BaseSpider):</p>

<pre><code>name = "shiyifang"
allowed_domains = ["s.taobao.com"]
start_urls = [
    "http://s.taobao.com/search?q=%E6%A0%B8%E6%A1%83"
]

def parse(self, response):
    filename = response.url.split("/")[-2]
    open(filename, 'wb').write(response.body)
</code></pre>

<p>```</p>

<h1>Crawling</h1>

<p>到项目的顶层目录，执行以下的指令：
```
$ scrapy crawl shiyifang
2014-04-28 23:01:34+0800 [scrapy] INFO: Scrapy 0.20.0 started (bot: shiyifang)
2014-04-28 23:01:34+0800 [scrapy] DEBUG: Optional features available: ssl, http11
2014-04-28 23:01:34+0800 [scrapy] DEBUG: Overridden settings: {&lsquo;NEWSPIDER_MODULE&rsquo;: &lsquo;shiyifang.spiders&rsquo;, &lsquo;SPIDER_MODULES&rsquo;: [&lsquo;shiyifang.spiders&rsquo;], &lsquo;BOT_NAME&rsquo;: &lsquo;shiyifang&rsquo;}
2014-04-28 23:01:35+0800 [scrapy] DEBUG: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2014-04-28 23:01:35+0800 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2014-04-28 23:01:35+0800 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2014-04-28 23:01:35+0800 [scrapy] DEBUG: Enabled item pipelines:
2014-04-28 23:01:35+0800 [shiyifang] INFO: Spider opened
2014-04-28 23:01:35+0800 [shiyifang] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2014-04-28 23:01:35+0800 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023
2014-04-28 23:01:35+0800 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080
2014-04-28 23:01:36+0800 [shiyifang] DEBUG: Crawled (200) <GET http://s.taobao.com/search?q=%E6%A0%B8%E6%A1%83> (referer: None)
2014-04-28 23:01:36+0800 [shiyifang] INFO: Closing spider (finished)
2014-04-28 23:01:36+0800 [shiyifang] INFO: Dumping Scrapy stats:</p>

<pre><code>{'downloader/request_bytes': 245,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 34569,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2014, 4, 28, 15, 1, 36, 232914),
 'log_count/DEBUG': 7,
 'log_count/INFO': 3,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2014, 4, 28, 15, 1, 35, 615729)}
</code></pre>

<p>2014-04-28 23:01:36+0800 [shiyifang] INFO: Spider closed (finished)
```
从debug信息可以看出，我们发送的get请求是收到回复的，且是成功的。此时，在当前目录下你会看到一个叫s.taobao.com的文件生成，这就是我们抓取到的网页。</p>

<p><big>参考文章</big></p>

<p><a href="http://doc.scrapy.org/en/latest/intro/tutorial.html">Scrapy Tutorial</a><br/>
<a href="http://blog.pluskid.org/?p=366">Scrapy 轻松定制网络爬虫</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[运行Scrapy项目出错：KeyError: 'Spider not found: tutorial']]></title>
    <link href="http://txgcwm.github.io/blog/2014/04/27/yun-xing-scrapyxiang-mu-chu-cuo/"/>
    <updated>2014-04-27T22:40:00+08:00</updated>
    <id>http://txgcwm.github.io/blog/2014/04/27/yun-xing-scrapyxiang-mu-chu-cuo</id>
    <content type="html"><![CDATA[<p>运行Scrapy抓取网上数据的时候，出现了以下的错误：
```
$ sudo scrapy crawl tutorial
Traceback (most recent call last):
  File &ldquo;/usr/local/bin/scrapy&rdquo;, line 4, in <module></p>

<pre><code>execute()
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py&rdquo;, line 143, in execute</p>

<pre><code>_run_print_help(parser, _run_command, cmd, args, opts)
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py&rdquo;, line 89, in _run_print_help</p>

<pre><code>func(*a, **kw)
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py&rdquo;, line 150, in _run_command</p>

<pre><code>cmd.run(args, opts)
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py&rdquo;, line 48, in run</p>

<pre><code>spider = crawler.spiders.create(spname, **opts.spargs)
</code></pre>

<p>  File &ldquo;/usr/local/lib/python2.7/dist-packages/scrapy/spidermanager.py&rdquo;, line 44, in create</p>

<pre><code>raise KeyError("Spider not found: %s" % spider_name)
</code></pre>

<p>KeyError: &lsquo;Spider not found: tutorial&rsquo;
```</p>

<!--more-->


<p>查看<code>tutorial/spiders/dmoz_spider.py</code>文件中的内容如下：
```
$ cat tutorial/spiders/dmoz_spider.py
from scrapy.spider import BaseSpider</p>

<p>class DmozSpider(BaseSpider):</p>

<pre><code>name = "dmoz"
allowed_domains = ["s.taobao.com"]
start_urls = [
    "http://s.taobao.com/search?q=%E6%A0%B8%E6%A1%83"
]

def parse(self, response):
    filename = response.url.split("/")[-2]
    open(filename, 'wb').write(response.body)
</code></pre>

<p>```</p>

<p><code>name = "dmoz"</code>中的<code>dmoz</code>才是crawler的名字。修改crawler的名字后，正常运行如下：
```
$ sudo scrapy crawl dmoz
2014-04-27 22:50:09+0800 [scrapy] INFO: Scrapy 0.20.0 started (bot: tutorial)
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Optional features available: ssl, http11
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Overridden settings: {&lsquo;NEWSPIDER_MODULE&rsquo;: &lsquo;tutorial.spiders&rsquo;, &lsquo;SPIDER_MODULES&rsquo;: [&lsquo;tutorial.spiders&rsquo;], &lsquo;BOT_NAME&rsquo;: &lsquo;tutorial&rsquo;}
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Enabled item pipelines:
2014-04-27 22:50:09+0800 [dmoz] INFO: Spider opened
2014-04-27 22:50:09+0800 [dmoz] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023
2014-04-27 22:50:09+0800 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080
2014-04-27 22:50:10+0800 [dmoz] DEBUG: Crawled (200) <GET http://s.taobao.com/search?q=%E6%A0%B8%E6%A1%83> (referer: None)
2014-04-27 22:50:10+0800 [dmoz] INFO: Closing spider (finished)
2014-04-27 22:50:10+0800 [dmoz] INFO: Dumping Scrapy stats:</p>

<pre><code>{'downloader/request_bytes': 245,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 35012,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2014, 4, 27, 14, 50, 10, 637578),
 'log_count/DEBUG': 7,
 'log_count/INFO': 3,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2014, 4, 27, 14, 50, 9, 940145)}
</code></pre>

<p>2014-04-27 22:50:10+0800 [dmoz] INFO: Spider closed (finished)
```</p>

<p><big>参考文章：</big></p>

<p><a href="http://www.crifan.com/python_run_scrapy_keyerror_spider_not_found/">运行Scrapy项目结果出错：KeyError: ‘Spider not found: manta’</a><br/>
<a href="http://www.crifan.com/try_scrapy_tutorial/">折腾Scrapy的Tutorial</a><br/>
<a href="http://stackoverflow.com/questions/9876793/scrapy-spider-not-found-error">Scrapy spider not found error</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu12.10安装Scrapy]]></title>
    <link href="http://txgcwm.github.io/blog/2013/11/13/ubuntu12-dot-10an-zhuang-scrapy/"/>
    <updated>2013-11-13T15:59:00+08:00</updated>
    <id>http://txgcwm.github.io/blog/2013/11/13/ubuntu12-dot-10an-zhuang-scrapy</id>
    <content type="html"><![CDATA[<p>根据官网<a href="http://doc.scrapy.org/en/latest/topics/ubuntu.html#topics-ubuntu">Ubuntu packages</a>上的方法安装Scrapy，然后创建一个教程目录，却出现了一些问题，如下所示。</p>

<p>```
$ scrapy startproject tutorial<br/>
Traceback (most recent call last):<br/>
  File &ldquo;/usr/bin/scrapy&rdquo;, line 4, in <module></p>

<pre><code>execute()  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/cmdline.py&rdquo;, line 121, in execute</p>

<pre><code>cmds = _get_commands_dict(settings, inproject)  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/cmdline.py&rdquo;, line 45, in _get_commands_dict</p>

<pre><code>cmds = _get_commands_from_module('scrapy.commands', inproject)  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/cmdline.py&rdquo;, line 28, in _get_commands_from_module</p>

<pre><code>for cmd in _iter_command_classes(module):  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/cmdline.py&rdquo;, line 19, in _iter_command_classes</p>

<pre><code>for module in walk_modules(module_name):  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/utils/misc.py&rdquo;, line 66, in walk_modules</p>

<pre><code>submod = __import__(fullpath, {}, {}, [''])  
</code></pre>

<p>  File &ldquo;/usr/lib/pymodules/python2.7/scrapy/commands/deploy.py&rdquo;, line 13, in <module></p>

<pre><code>from w3lib.form import encode_multipart  
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/w3lib/form.py&rdquo;, line 2, in <module></p>

<pre><code>if six.PY2:  
</code></pre>

<p>AttributeError: &lsquo;module&rsquo; object has no attribute &lsquo;PY2'<br/>
```</p>

<!--more-->


<p>从网上找到了一个答案，需要利用pip去安装。</p>

<p>```
$ pip install Scrapy<br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): Scrapy in   /usr/lib/pymodules/python2.7 <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): Twisted>=8.0 in   /usr/lib/python2.7/dist-packages (from Scrapy)<br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): w3lib>=1.2 in   /usr/lib/python2.7/dist-packages (from Scrapy)<br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): queuelib in   /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): lxml in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): pyOpenSSL in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Downloading/unpacking six>=1.4.1 (from w3lib>=1.2->Scrapy) <br/>
  Downloading six-1.4.1.tar.gz <br/>
  Running setup.py egg_info for package six</p>

<p>Installing collected packages: six <br/>
  Found existing installation: six 1.1.0</p>

<pre><code>Uninstalling six:   
</code></pre>

<p>Exception: <br/>
Traceback (most recent call last): <br/>
  File &ldquo;/usr/lib/python2.7/dist-packages/pip/basecommand.py&rdquo;, line 104, in main</p>

<pre><code>status = self.run(options, args)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/commands/install.py&rdquo;, line 250, in run</p>

<pre><code>requirement_set.install(install_options, global_options)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/req.py&rdquo;, line 1129, in install</p>

<pre><code>requirement.uninstall(auto_confirm=True)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/req.py&rdquo;, line 486, in uninstall</p>

<pre><code>paths_to_remove.remove(auto_confirm)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/req.py&rdquo;, line 1431, in remove</p>

<pre><code>renames(path, new_path)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/dist-packages/pip/util.py&rdquo;, line 263, in renames</p>

<pre><code>shutil.move(old, new)   
</code></pre>

<p>  File &ldquo;/usr/lib/python2.7/shutil.py&rdquo;, line 302, in move</p>

<pre><code>os.unlink(src)   
</code></pre>

<p>OSError: [Errno 13] Permission denied: &lsquo;/usr/share/pyshared/six-1.1.0.egg-info&rsquo;</p>

<p>Storing complete log in /home/txgcwm/.pip/pip.log <br/>
```</p>

<p>借助新得立把six给卸载掉，然后重新安装。</p>

<p>```
$ sudo pip install Scrapy<br/>
Downloading/unpacking Scrapy <br/>
  Downloading Scrapy-0.20.0.tar.gz (745Kb): 745Kb downloaded <br/>
  Running setup.py egg_info for package Scrapy</p>

<pre><code>no previously-included directories found matching 'docs/build'   
</code></pre>

<p>Requirement already satisfied (use &mdash;upgrade to upgrade): Twisted>=10.0.0 in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Downloading/unpacking w3lib>=1.2 (from Scrapy) <br/>
  Downloading w3lib-1.5.tar.gz <br/>
  Running setup.py egg_info for package w3lib</p>

<p>Requirement already satisfied (use &mdash;upgrade to upgrade): queuelib in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): lxml in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Requirement already satisfied (use &mdash;upgrade to upgrade): pyOpenSSL in    /usr/lib/python2.7/dist-packages (from Scrapy) <br/>
Downloading/unpacking cssselect>=0.9 (from Scrapy) <br/>
  Downloading cssselect-0.9.1.tar.gz <br/>
  Running setup.py egg_info for package cssselect</p>

<pre><code>no previously-included directories found matching 'docs/_build'   
</code></pre>

<p>Downloading/unpacking six>=1.4.1 (from w3lib>=1.2->Scrapy) <br/>
  Running setup.py egg_info for package six</p>

<p>Installing collected packages: Scrapy, w3lib, cssselect, six <br/>
  Running setup.py install for Scrapy</p>

<pre><code>changing mode of build/scripts-2.7/scrapy from 644 to 755   

no previously-included directories found matching 'docs/build'   
changing mode of /usr/local/bin/scrapy to 755   
</code></pre>

<p>  Running setup.py install for w3lib</p>

<p>  Running setup.py install for cssselect</p>

<pre><code>no previously-included directories found matching 'docs/_build'   
</code></pre>

<p>  Running setup.py install for six</p>

<p>Successfully installed Scrapy w3lib cssselect six <br/>
Cleaning up&hellip; <br/>
```</p>

<p>创立一个项目。</p>

<p><code>
$ sudo scrapy startproject tutorial   
</code></p>
]]></content>
  </entry>
  
</feed>
